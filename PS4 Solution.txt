* Problem set 4 - Problem 1 
clear all

* 1) Generate the database: 1000 draws from the uniform (x1) and the chi2(1) distribution.
set obs 1000
gen x1 = uniform()
gen u = rchi2(1) - 1
hist x1, name(plot_x1, replace)
hist u, name(plot_u, replace)

* 2) Clear all data.
clear all

* Define a program to generate 5 observations from the chi2(1) distribution and save mean and standard deviation of the generated sample.
program define mysim, rclass
drop _all
** Note difference between drop _all and clear.
set obs 5
gen u = rchi2(1) - 1
sum u
return scalar mean = r(mean)
return scalar sd = r(sd)
end
** The program can now be used as a command.

* Simulate the program 10,000 times to obtain a dataset with a variable for the mean and one for the standard deviation.
help simulate
simulate umean = r(mean) usd = r(sd), reps(10000): mysim

* Summarize and plot a histogram of the mean variable.
sum umean
hist umean, name(hist_umean, replace)

* 3) Repeat with programs for N=10, N=100, N=1000
program define mysimloop, rclass
drop _all
foreach i of numlist 10 100 1000{
	set obs `i'
	gen u`i'=rchi2(1)-1
	sum u`i'
	return scalar mean`i'=r(mean)
}
end

* Calculate the summary statistics of each of the mean variables obtained. What do you observe?
simulate umean10=r(mean10) umean100=r(mean100) umean1000=r(mean1000), reps(10000): mysimloop
sum umean*
** Note: the number of observations written in the summary table corresponds to the number of simulations.
** As the number of observations increases, the standard deviation decreases and the confidence interval gets smaller around 0 (the true value). You can plot them to see the convergence to the normal distribution:
hist umean10, normal name(hist_umean10, replace)
hist umean100, normal name(hist_umean100, replace)
hist umean1000, normal name(hist_umean1000, replace)
** central limit theorem: average tends to a normal distribution

* 4)
sum umean10
display r(sd)*(sqrt(10)/sqrt(100))
** Gives sd of umean100

display r(sd)*(sqrt(10)/sqrt(1000))
** Gives sd of umean1000

sum umean*
** Standard deviation converges to 0 at rate sqrt(N): if I have a sample of size N and multiply it by 10, sd10N = sdN * (sqrt(10)/sqrt(10N))

* What sample size would you need to obtain a standard deviation of about 0.001?
** To get sd=0.001, one needs to find N' such that 0.001 = sdN * (sqrt(10)/sqrt(N'))
** Equivalent to sqrt(N') = sdN * sqrt(10) / 0.001
** Equivalent to N' = (sdN * sqrt(10) / 0.001)^2
display (r(sd) * sqrt(10)/0.001)^2
* Need a very large sample size: more than 2 million observations!

* 5) Same simulations, looking at histograms:
hist umean10, normal
graph save umean10, replace
hist umean100, normal
graph save umean100, replace
hist umean1000, normal
graph save umean1000, replace
graph combine umean10.gph umean100.gph umean1000.gph
** As N increases, the distribution of umean becomes more normal. (CLT)

* 6) and 7) Create a new program
capture program drop mysim2
program define mysim2, rclass
drop _all
foreach i of numlist 10 1000{
	set obs `i'
	gen x1`i'=runiform()
	gen x2`i'=rbinomial(1,0.3)
	gen y`i'=1+2*x1`i'+10*x2`i'+(rchi2(1)-1)
	reg y`i' x1`i' x2`i'
	return scalar b0`i'=_b[_cons]
	return scalar b1`i'=_b[x1`i']
	return scalar b2`i'=_b[x2`i']
	return scalar se0`i'=_se[_cons]
	return scalar se1`i'=_se[x1`i']
	return scalar se2`i'=_se[x2`i']
}
end 

simulate b010=r(b010) b110=r(b110) b210=r(b210) ///
		 se010=r(se010) se110=r(se110) se210=r(se210) ///
		 b01000=r(b01000) b11000=r(b11000) b21000=r(b21000) ///
		 se01000=r(se01000) se11000=r(se11000) se21000=r(se21000), reps(10000): mysim2
* Verify that your OLS estimates converge to the population values. Is there a way to verify whether your estimated standard errors are consistent?
sum
** As N increases, the OLS estimator should converge to the true values, which are 1, 2 and 10. Moreover, the variance of the OLS estimator should tend to 0. We see that standard deviations are divided by 10 between the 10 observations simulation and the 1000 observations simulation. It shows again that standard deviations converge to 0 at rate sqrt(N).

* Also inspect their distributions as above, and compare them to the normal distribution.
** Graphs for b0
hist b010, normal
graph save b010, replace
hist b01000, normal
graph save b01000, replace
** Graphs for b1
hist b110, normal
graph save b110, replace
hist b11000, normal
graph save b11000, replace
** Graphs for b2
hist b210, normal
graph save b210, replace
hist b21000, normal
graph save b21000, replace
graph combine b010.gph b01000.gph b110.gph b11000.gph b210.gph b21000.gph

* 8)
capture program drop mysim3
program define mysim3, rclass
drop _all
foreach i of numlist 10 1000{
	set obs `i'
	gen x1`i'=runiform()
	gen x2`i'=rbinomial(1,0.3)
	gen u`i'=rchi2(1)-1
	gen y`i'	= 1 + 2*x1`i' + 10*x2`i' + u`i' 				
		
	reg y`i' x1`i' x2`i'
	test (_cons=1) (x1`i'=2)
	return scalar F`i' = r(F)
}
end

simulate F10=r(F10) F1000=r(F1000), reps(10000): mysim3

* Summarize the dataset obtained and plot a histogram of statistics
sum
hist F10
hist F1000
** The aim here is to show that the F test is valid when N is large, if the error term does not follow a normal distribution. 
** Since b0=1 and b1=2, we know that we should not be rejecting the null hypothesis.

* 9) Evaluate the precision of Monte Carlo simulations through the following:
* What are the critical values at the 1 and 5 percent significance level above which the test hypothesis can be rejected?
** The test includes 2 hypothesis here (b0 = 1 and b1 = 2). With 10 (respectively 1,000) observations and 3 parameters to estimate, the degree of freedom is 7 (respectively 997). It follows that the critical values are:
display invFtail(2,7,0.01)
display invFtail(2,7,0.05)
display invFtail(2,997,0.01)
display invFtail(2,997,0.05)

* Count the number of observations that exceed the critical value of the 1 and 5 percent significance level test. What do you observe?
count if F10 > invFtail(2,7,0.01)
count if F10 > invFtail(2,7,0.05)
count if F1000 > invFtail(2,997,0.01)
count if F1000 > invFtail(2,997,0.05)
** I observe that the number of observations exceeding the critical values decreases when N increases.
** Put otherwise, it is less likely to wrongly reject the null hypothesis (type I error).

* How many observations should we have observed to exceed the critical value, had the distribution of the error terms been normal (i.e. F stat followed an F distribution)?
display _N*Ftail(2,7,invFtail(2,7,0.01))
display _N*Ftail(2,7,invFtail(2,7,0.05))
display _N*Ftail(2,997,invFtail(2,997,0.01))
display _N*Ftail(2,997,invFtail(2,997,0.05))

* How does this compare to what we observed in the Monte Carlo simulations?
** Far from what is observed in the 10 obs simulation. However when N is large, it is very close to what we would have observed with normally-distributed error terms.

